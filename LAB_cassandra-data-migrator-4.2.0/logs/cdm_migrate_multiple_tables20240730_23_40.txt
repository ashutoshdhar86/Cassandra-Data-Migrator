24/07/30 23:40:46 INFO Migrate$: ################################################################################################
24/07/30 23:40:46 INFO Migrate$: ###                                  Migrate Job - Starting                                  ###
24/07/30 23:40:46 INFO Migrate$: ################################################################################################
24/07/30 23:40:46 INFO SparkContext: Running Spark version 3.5.1
24/07/30 23:40:46 INFO SparkContext: OS info Linux, 4.18.0-553.6.1.el8.x86_64, amd64
24/07/30 23:40:46 INFO SparkContext: Java version 11.0.20.1
24/07/30 23:40:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
24/07/30 23:40:46 INFO ResourceUtils: ==============================================================
24/07/30 23:40:46 INFO ResourceUtils: No custom resources configured for spark.driver.
24/07/30 23:40:46 INFO ResourceUtils: ==============================================================
24/07/30 23:40:46 INFO SparkContext: Submitted application: Migrate Job
24/07/30 23:40:46 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
24/07/30 23:40:46 INFO ResourceProfile: Limiting resource is cpu
24/07/30 23:40:46 INFO ResourceProfileManager: Added ResourceProfile id: 0
24/07/30 23:40:46 INFO SecurityManager: Changing view acls to: automaton
24/07/30 23:40:46 INFO SecurityManager: Changing modify acls to: automaton
24/07/30 23:40:46 INFO SecurityManager: Changing view acls groups to: 
24/07/30 23:40:46 INFO SecurityManager: Changing modify acls groups to: 
24/07/30 23:40:46 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: automaton; groups with view permissions: EMPTY; users with modify permissions: automaton; groups with modify permissions: EMPTY
24/07/30 23:40:47 INFO Utils: Successfully started service 'sparkDriver' on port 45591.
24/07/30 23:40:47 INFO SparkEnv: Registering MapOutputTracker
24/07/30 23:40:47 INFO SparkEnv: Registering BlockManagerMaster
24/07/30 23:40:47 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
24/07/30 23:40:47 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
24/07/30 23:40:47 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
24/07/30 23:40:47 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-be3a93a9-4a6b-478f-acd9-188bec8affbc
24/07/30 23:40:47 INFO MemoryStore: MemoryStore started with capacity 1048.8 MiB
24/07/30 23:40:47 INFO SparkEnv: Registering OutputCommitCoordinator
24/07/30 23:40:47 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
24/07/30 23:40:47 INFO Utils: Successfully started service 'SparkUI' on port 4040.
24/07/30 23:40:47 INFO SparkContext: Added JAR file:/home/automaton/cassandra-data-migrator-4.2.0.jar at spark://ip-10-166-69-127.us-west-2.compute.internal:45591/jars/cassandra-data-migrator-4.2.0.jar with timestamp 1722382846228
24/07/30 23:40:47 INFO Executor: Starting executor ID driver on host ip-10-166-69-127.us-west-2.compute.internal
24/07/30 23:40:47 INFO Executor: OS info Linux, 4.18.0-553.6.1.el8.x86_64, amd64
24/07/30 23:40:47 INFO Executor: Java version 11.0.20.1
24/07/30 23:40:47 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
24/07/30 23:40:47 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@67507df for default.
24/07/30 23:40:47 INFO Executor: Fetching spark://ip-10-166-69-127.us-west-2.compute.internal:45591/jars/cassandra-data-migrator-4.2.0.jar with timestamp 1722382846228
24/07/30 23:40:47 INFO TransportClientFactory: Successfully created connection to ip-10-166-69-127.us-west-2.compute.internal/10.166.69.127:45591 after 46 ms (0 ms spent in bootstraps)
24/07/30 23:40:47 INFO Utils: Fetching spark://ip-10-166-69-127.us-west-2.compute.internal:45591/jars/cassandra-data-migrator-4.2.0.jar to /tmp/spark-33c3d76f-cbde-49a5-b43d-0dd705a6b484/userFiles-d83a5729-9fb0-4bbe-9925-c6e445187a9e/fetchFileTemp8186757226019518767.tmp
24/07/30 23:40:48 INFO Executor: Adding file:/tmp/spark-33c3d76f-cbde-49a5-b43d-0dd705a6b484/userFiles-d83a5729-9fb0-4bbe-9925-c6e445187a9e/cassandra-data-migrator-4.2.0.jar to class loader default
24/07/30 23:40:48 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38623.
24/07/30 23:40:48 INFO NettyBlockTransferService: Server created on ip-10-166-69-127.us-west-2.compute.internal:38623
24/07/30 23:40:48 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
24/07/30 23:40:48 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, ip-10-166-69-127.us-west-2.compute.internal, 38623, None)
24/07/30 23:40:48 INFO BlockManagerMasterEndpoint: Registering block manager ip-10-166-69-127.us-west-2.compute.internal:38623 with 1048.8 MiB RAM, BlockManagerId(driver, ip-10-166-69-127.us-west-2.compute.internal, 38623, None)
24/07/30 23:40:48 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, ip-10-166-69-127.us-west-2.compute.internal, 38623, None)
24/07/30 23:40:48 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, ip-10-166-69-127.us-west-2.compute.internal, 38623, None)
24/07/30 23:40:48 INFO PropertyHelper: Processing explicitly set and known sparkConf properties
24/07/30 23:40:48 INFO PropertyHelper: Known property [spark.cdm.autocorrect.missing] is configured with value [false] and is type [BOOLEAN]
24/07/30 23:40:48 INFO PropertyHelper: Known property [spark.cdm.schema.origin.keyspaceTable] is configured with value [ks1.cdm,keyspace2.testtable] and is type [STRING]
24/07/30 23:40:48 INFO PropertyHelper: Known property [spark.cdm.connect.origin.host] is configured with value [10.166.65.33] and is type [STRING]
24/07/30 23:40:48 INFO PropertyHelper: Known property [spark.cdm.perfops.numParts] is configured with value [10] and is type [NUMBER]
24/07/30 23:40:48 INFO PropertyHelper: Known property [spark.cdm.connect.origin.password] is configured with value [********] and is type [STRING]
24/07/30 23:40:48 INFO PropertyHelper: Known property [spark.cdm.connect.target.password] is configured with value [********] and is type [STRING]
24/07/30 23:40:48 INFO PropertyHelper: Known property [spark.cdm.connect.target.username] is configured with value [cassandra] and is type [STRING]
24/07/30 23:40:48 INFO PropertyHelper: Known property [spark.cdm.autocorrect.mismatch] is configured with value [false] and is type [BOOLEAN]
24/07/30 23:40:48 INFO PropertyHelper: Known property [spark.cdm.connect.target.host] is configured with value [10.166.65.248] and is type [STRING]
24/07/30 23:40:48 INFO PropertyHelper: Known property [spark.cdm.connect.target.port] is configured with value [9042] and is type [NUMBER]
24/07/30 23:40:48 INFO PropertyHelper: Known property [spark.cdm.connect.origin.port] is configured with value [9042] and is type [NUMBER]
24/07/30 23:40:48 INFO PropertyHelper: Known property [spark.cdm.connect.origin.username] is configured with value [cassandra] and is type [STRING]
24/07/30 23:40:48 INFO PropertyHelper: Adding any missing known properties that have default values
24/07/30 23:40:48 INFO ConnectionFetcher: PARAM --  SSL Enabled: false
24/07/30 23:40:48 INFO ConnectionFetcher: Connecting to ORIGIN at 10.166.65.33:9042
24/07/30 23:40:50 INFO ConnectionFetcher: PARAM --  SSL Enabled: false
24/07/30 23:40:50 INFO ConnectionFetcher: Connecting to TARGET at 10.166.65.248:9042
24/07/30 23:40:50 INFO DefaultMavenCoordinates: Apache Cassandra Java Driver (org.apache.cassandra:java-driver-core-shaded) version 4.18.1
24/07/30 23:40:50 INFO CqlPrepareAsyncProcessor: Adding handler to invalidate cached prepared statements on type changes
24/07/30 23:40:51 INFO Clock: Using native clock for microsecond precision
24/07/30 23:40:51 WARN PlainTextAuthProviderBase: [] /10.166.65.33:9042 did not send an authentication challenge; This is suspicious because the driver expects authentication
24/07/30 23:40:51 WARN DefaultTopologyMonitor: [s0] Unable to determine broadcast RPC port.  Trying to fall back to port used by the control connection.
24/07/30 23:40:51 WARN PlainTextAuthProviderBase: [] /10.166.65.33:9042 did not send an authentication challenge; This is suspicious because the driver expects authentication
24/07/30 23:40:52 INFO CassandraConnector: Connected to Cassandra cluster.
24/07/30 23:40:52 INFO Migrate$: PARAM -- Min Partition: -9223372036854775808
24/07/30 23:40:52 INFO Migrate$: PARAM -- Max Partition: 9223372036854775807
24/07/30 23:40:52 INFO Migrate$: PARAM -- Number of Splits : 10
24/07/30 23:40:52 INFO Migrate$: PARAM -- Coverage Percent: 100
24/07/30 23:40:52 INFO SplitPartitions: ThreadID: 1 Splitting min: -9223372036854775808 max: 9223372036854775807
24/07/30 23:40:52 INFO Migrate$: PARAM Calculated -- Total Partitions: 10
24/07/30 23:40:52 INFO Migrate$: Spark parallelize created : 10 slices!
24/07/30 23:40:52 INFO SparkContext: Starting job: foreach at Migrate.scala:24
24/07/30 23:40:52 INFO DAGScheduler: Got job 0 (foreach at Migrate.scala:24) with 10 output partitions
24/07/30 23:40:52 INFO DAGScheduler: Final stage: ResultStage 0 (foreach at Migrate.scala:24)
24/07/30 23:40:52 INFO DAGScheduler: Parents of final stage: List()
24/07/30 23:40:52 INFO DAGScheduler: Missing parents: List()
24/07/30 23:40:52 INFO DAGScheduler: Submitting ResultStage 0 (ParallelCollectionRDD[0] at parallelize at BaseJob.scala:90), which has no missing parents
24/07/30 23:40:53 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 3.1 KiB, free 1048.8 MiB)
24/07/30 23:40:53 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1854.0 B, free 1048.8 MiB)
24/07/30 23:40:53 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on ip-10-166-69-127.us-west-2.compute.internal:38623 (size: 1854.0 B, free: 1048.8 MiB)
24/07/30 23:40:53 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1585
24/07/30 23:40:53 INFO DAGScheduler: Submitting 10 missing tasks from ResultStage 0 (ParallelCollectionRDD[0] at parallelize at BaseJob.scala:90) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9))
24/07/30 23:40:53 INFO TaskSchedulerImpl: Adding task set 0.0 with 10 tasks resource profile 0
24/07/30 23:40:53 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (ip-10-166-69-127.us-west-2.compute.internal, executor driver, partition 0, PROCESS_LOCAL, 8324 bytes) 
24/07/30 23:40:53 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (ip-10-166-69-127.us-west-2.compute.internal, executor driver, partition 1, PROCESS_LOCAL, 8324 bytes) 
24/07/30 23:40:53 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
24/07/30 23:40:53 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
24/07/30 23:40:53 INFO CqlPrepareAsyncProcessor: Adding handler to invalidate cached prepared statements on type changes
24/07/30 23:40:53 INFO Clock: Using native clock for microsecond precision
24/07/30 23:40:53 WARN PlainTextAuthProviderBase: [] /10.166.65.248:9042 did not send an authentication challenge; This is suspicious because the driver expects authentication
24/07/30 23:40:53 WARN PlainTextAuthProviderBase: [] /10.166.65.248:9042 did not send an authentication challenge; This is suspicious because the driver expects authentication
24/07/30 23:40:53 INFO CassandraConnector: Connected to Cassandra cluster.
24/07/30 23:40:53 INFO CopyJobSession: PARAM -- Max Retries: 0
24/07/30 23:40:53 INFO CopyJobSession: PARAM -- Partition file input: ./ks1.cdm,keyspace2.testtable_partitions.csv
24/07/30 23:40:53 INFO CopyJobSession: PARAM -- Partition file output: ./ks1.cdm,keyspace2.testtable_partitions.csv
24/07/30 23:40:53 INFO CopyJobSession: PARAM -- Origin Rate Limit: 20000.0
24/07/30 23:40:53 INFO CopyJobSession: PARAM -- Target Rate Limit: 40000.0
24/07/30 23:40:53 INFO CopyJobSession: PARAM -- Max Retries: 0
24/07/30 23:40:53 INFO CopyJobSession: PARAM -- Partition file input: ./ks1.cdm,keyspace2.testtable_partitions.csv
24/07/30 23:40:53 INFO CopyJobSession: PARAM -- Partition file output: ./ks1.cdm,keyspace2.testtable_partitions.csv
24/07/30 23:40:53 INFO CopyJobSession: PARAM -- Origin Rate Limit: 20000.0
24/07/30 23:40:53 INFO CopyJobSession: PARAM -- Target Rate Limit: 40000.0
24/07/30 23:40:53 ERROR Executor: Exception in task 0.0 in stage 0.0 (TID 0)
java.lang.IllegalArgumentException: Table not found: cdm,keyspace2
	at com.datastax.cdm.schema.CqlTable.setCqlMetadata(CqlTable.java:385)
	at com.datastax.cdm.schema.CqlTable.<init>(CqlTable.java:99)
	at com.datastax.cdm.cql.EnhancedSession.<init>(EnhancedSession.java:51)
	at com.datastax.cdm.job.AbstractJobSession.<init>(AbstractJobSession.java:72)
	at com.datastax.cdm.job.AbstractJobSession.<init>(AbstractJobSession.java:44)
	at com.datastax.cdm.job.CopyJobSession.<init>(CopyJobSession.java:47)
	at com.datastax.cdm.job.CopyJobSessionFactory.getInstance(CopyJobSessionFactory.java:28)
	at com.datastax.cdm.job.Migrate$.$anonfun$execute$3(Migrate.scala:27)
	at com.datastax.cdm.job.Migrate$.$anonfun$execute$3$adapted(Migrate.scala:26)
	at com.datastax.spark.connector.cql.CassandraConnector.$anonfun$withSessionDo$1(CassandraConnector.scala:104)
	at com.datastax.spark.connector.cql.CassandraConnector.closeResourceAfterUse(CassandraConnector.scala:121)
	at com.datastax.spark.connector.cql.CassandraConnector.withSessionDo(CassandraConnector.scala:103)
	at com.datastax.cdm.job.Migrate$.$anonfun$execute$2(Migrate.scala:26)
	at com.datastax.cdm.job.Migrate$.$anonfun$execute$2$adapted(Migrate.scala:25)
	at com.datastax.spark.connector.cql.CassandraConnector.$anonfun$withSessionDo$1(CassandraConnector.scala:104)
	at com.datastax.spark.connector.cql.CassandraConnector.closeResourceAfterUse(CassandraConnector.scala:121)
	at com.datastax.spark.connector.cql.CassandraConnector.withSessionDo(CassandraConnector.scala:103)
	at com.datastax.cdm.job.Migrate$.$anonfun$execute$1(Migrate.scala:25)
	at com.datastax.cdm.job.Migrate$.$anonfun$execute$1$adapted(Migrate.scala:24)
	at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
	at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2(RDD.scala:1031)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2$adapted(RDD.scala:1031)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/07/30 23:40:53 ERROR Executor: Exception in task 1.0 in stage 0.0 (TID 1)
java.lang.IllegalArgumentException: Table not found: cdm,keyspace2
	at com.datastax.cdm.schema.CqlTable.setCqlMetadata(CqlTable.java:385)
	at com.datastax.cdm.schema.CqlTable.<init>(CqlTable.java:99)
	at com.datastax.cdm.cql.EnhancedSession.<init>(EnhancedSession.java:51)
	at com.datastax.cdm.job.AbstractJobSession.<init>(AbstractJobSession.java:72)
	at com.datastax.cdm.job.AbstractJobSession.<init>(AbstractJobSession.java:44)
	at com.datastax.cdm.job.CopyJobSession.<init>(CopyJobSession.java:47)
	at com.datastax.cdm.job.CopyJobSessionFactory.getInstance(CopyJobSessionFactory.java:28)
	at com.datastax.cdm.job.Migrate$.$anonfun$execute$3(Migrate.scala:27)
	at com.datastax.cdm.job.Migrate$.$anonfun$execute$3$adapted(Migrate.scala:26)
	at com.datastax.spark.connector.cql.CassandraConnector.$anonfun$withSessionDo$1(CassandraConnector.scala:104)
	at com.datastax.spark.connector.cql.CassandraConnector.closeResourceAfterUse(CassandraConnector.scala:121)
	at com.datastax.spark.connector.cql.CassandraConnector.withSessionDo(CassandraConnector.scala:103)
	at com.datastax.cdm.job.Migrate$.$anonfun$execute$2(Migrate.scala:26)
	at com.datastax.cdm.job.Migrate$.$anonfun$execute$2$adapted(Migrate.scala:25)
	at com.datastax.spark.connector.cql.CassandraConnector.$anonfun$withSessionDo$1(CassandraConnector.scala:104)
	at com.datastax.spark.connector.cql.CassandraConnector.closeResourceAfterUse(CassandraConnector.scala:121)
	at com.datastax.spark.connector.cql.CassandraConnector.withSessionDo(CassandraConnector.scala:103)
	at com.datastax.cdm.job.Migrate$.$anonfun$execute$1(Migrate.scala:25)
	at com.datastax.cdm.job.Migrate$.$anonfun$execute$1$adapted(Migrate.scala:24)
	at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
	at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2(RDD.scala:1031)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2$adapted(RDD.scala:1031)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/07/30 23:40:53 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2) (ip-10-166-69-127.us-west-2.compute.internal, executor driver, partition 2, PROCESS_LOCAL, 8324 bytes) 
24/07/30 23:40:53 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
24/07/30 23:40:53 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3) (ip-10-166-69-127.us-west-2.compute.internal, executor driver, partition 3, PROCESS_LOCAL, 8324 bytes) 
24/07/30 23:40:53 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
24/07/30 23:40:53 WARN TaskSetManager: Lost task 1.0 in stage 0.0 (TID 1) (ip-10-166-69-127.us-west-2.compute.internal executor driver): java.lang.IllegalArgumentException: Table not found: cdm,keyspace2
	at com.datastax.cdm.schema.CqlTable.setCqlMetadata(CqlTable.java:385)
	at com.datastax.cdm.schema.CqlTable.<init>(CqlTable.java:99)
	at com.datastax.cdm.cql.EnhancedSession.<init>(EnhancedSession.java:51)
	at com.datastax.cdm.job.AbstractJobSession.<init>(AbstractJobSession.java:72)
	at com.datastax.cdm.job.AbstractJobSession.<init>(AbstractJobSession.java:44)
	at com.datastax.cdm.job.CopyJobSession.<init>(CopyJobSession.java:47)
	at com.datastax.cdm.job.CopyJobSessionFactory.getInstance(CopyJobSessionFactory.java:28)
	at com.datastax.cdm.job.Migrate$.$anonfun$execute$3(Migrate.scala:27)
	at com.datastax.cdm.job.Migrate$.$anonfun$execute$3$adapted(Migrate.scala:26)
	at com.datastax.spark.connector.cql.CassandraConnector.$anonfun$withSessionDo$1(CassandraConnector.scala:104)
	at com.datastax.spark.connector.cql.CassandraConnector.closeResourceAfterUse(CassandraConnector.scala:121)
	at com.datastax.spark.connector.cql.CassandraConnector.withSessionDo(CassandraConnector.scala:103)
	at com.datastax.cdm.job.Migrate$.$anonfun$execute$2(Migrate.scala:26)
	at com.datastax.cdm.job.Migrate$.$anonfun$execute$2$adapted(Migrate.scala:25)
	at com.datastax.spark.connector.cql.CassandraConnector.$anonfun$withSessionDo$1(CassandraConnector.scala:104)
	at com.datastax.spark.connector.cql.CassandraConnector.closeResourceAfterUse(CassandraConnector.scala:121)
	at com.datastax.spark.connector.cql.CassandraConnector.withSessionDo(CassandraConnector.scala:103)
	at com.datastax.cdm.job.Migrate$.$anonfun$execute$1(Migrate.scala:25)
	at com.datastax.cdm.job.Migrate$.$anonfun$execute$1$adapted(Migrate.scala:24)
	at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
	at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2(RDD.scala:1031)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2$adapted(RDD.scala:1031)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)

24/07/30 23:40:53 ERROR TaskSetManager: Task 1 in stage 0.0 failed 1 times; aborting job
24/07/30 23:40:53 INFO TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0) on ip-10-166-69-127.us-west-2.compute.internal, executor driver: java.lang.IllegalArgumentException (Table not found: cdm,keyspace2) [duplicate 1]
24/07/30 23:40:53 INFO CopyJobSession: PARAM -- Max Retries: 0
24/07/30 23:40:53 INFO CopyJobSession: PARAM -- Partition file input: ./ks1.cdm,keyspace2.testtable_partitions.csv
24/07/30 23:40:53 INFO CopyJobSession: PARAM -- Partition file output: ./ks1.cdm,keyspace2.testtable_partitions.csv
24/07/30 23:40:53 INFO CopyJobSession: PARAM -- Origin Rate Limit: 20000.0
24/07/30 23:40:53 INFO CopyJobSession: PARAM -- Target Rate Limit: 40000.0
24/07/30 23:40:53 ERROR Executor: Exception in task 3.0 in stage 0.0 (TID 3)
java.lang.IllegalArgumentException: Table not found: cdm,keyspace2
	at com.datastax.cdm.schema.CqlTable.setCqlMetadata(CqlTable.java:385)
	at com.datastax.cdm.schema.CqlTable.<init>(CqlTable.java:99)
	at com.datastax.cdm.cql.EnhancedSession.<init>(EnhancedSession.java:51)
	at com.datastax.cdm.job.AbstractJobSession.<init>(AbstractJobSession.java:72)
	at com.datastax.cdm.job.AbstractJobSession.<init>(AbstractJobSession.java:44)
	at com.datastax.cdm.job.CopyJobSession.<init>(CopyJobSession.java:47)
	at com.datastax.cdm.job.CopyJobSessionFactory.getInstance(CopyJobSessionFactory.java:28)
	at com.datastax.cdm.job.Migrate$.$anonfun$execute$3(Migrate.scala:27)
	at com.datastax.cdm.job.Migrate$.$anonfun$execute$3$adapted(Migrate.scala:26)
	at com.datastax.spark.connector.cql.CassandraConnector.$anonfun$withSessionDo$1(CassandraConnector.scala:104)
	at com.datastax.spark.connector.cql.CassandraConnector.closeResourceAfterUse(CassandraConnector.scala:121)
	at com.datastax.spark.connector.cql.CassandraConnector.withSessionDo(CassandraConnector.scala:103)
	at com.datastax.cdm.job.Migrate$.$anonfun$execute$2(Migrate.scala:26)
	at com.datastax.cdm.job.Migrate$.$anonfun$execute$2$adapted(Migrate.scala:25)
	at com.datastax.spark.connector.cql.CassandraConnector.$anonfun$withSessionDo$1(CassandraConnector.scala:104)
	at com.datastax.spark.connector.cql.CassandraConnector.closeResourceAfterUse(CassandraConnector.scala:121)
	at com.datastax.spark.connector.cql.CassandraConnector.withSessionDo(CassandraConnector.scala:103)
	at com.datastax.cdm.job.Migrate$.$anonfun$execute$1(Migrate.scala:25)
	at com.datastax.cdm.job.Migrate$.$anonfun$execute$1$adapted(Migrate.scala:24)
	at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
	at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2(RDD.scala:1031)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2$adapted(RDD.scala:1031)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/07/30 23:40:53 INFO CopyJobSession: PARAM -- Max Retries: 0
24/07/30 23:40:53 INFO CopyJobSession: PARAM -- Partition file input: ./ks1.cdm,keyspace2.testtable_partitions.csv
24/07/30 23:40:53 INFO CopyJobSession: PARAM -- Partition file output: ./ks1.cdm,keyspace2.testtable_partitions.csv
24/07/30 23:40:53 INFO CopyJobSession: PARAM -- Origin Rate Limit: 20000.0
24/07/30 23:40:53 INFO CopyJobSession: PARAM -- Target Rate Limit: 40000.0
24/07/30 23:40:53 ERROR Executor: Exception in task 2.0 in stage 0.0 (TID 2)
java.lang.IllegalArgumentException: Table not found: cdm,keyspace2
	at com.datastax.cdm.schema.CqlTable.setCqlMetadata(CqlTable.java:385)
	at com.datastax.cdm.schema.CqlTable.<init>(CqlTable.java:99)
	at com.datastax.cdm.cql.EnhancedSession.<init>(EnhancedSession.java:51)
	at com.datastax.cdm.job.AbstractJobSession.<init>(AbstractJobSession.java:72)
	at com.datastax.cdm.job.AbstractJobSession.<init>(AbstractJobSession.java:44)
	at com.datastax.cdm.job.CopyJobSession.<init>(CopyJobSession.java:47)
	at com.datastax.cdm.job.CopyJobSessionFactory.getInstance(CopyJobSessionFactory.java:28)
	at com.datastax.cdm.job.Migrate$.$anonfun$execute$3(Migrate.scala:27)
	at com.datastax.cdm.job.Migrate$.$anonfun$execute$3$adapted(Migrate.scala:26)
	at com.datastax.spark.connector.cql.CassandraConnector.$anonfun$withSessionDo$1(CassandraConnector.scala:104)
	at com.datastax.spark.connector.cql.CassandraConnector.closeResourceAfterUse(CassandraConnector.scala:121)
	at com.datastax.spark.connector.cql.CassandraConnector.withSessionDo(CassandraConnector.scala:103)
	at com.datastax.cdm.job.Migrate$.$anonfun$execute$2(Migrate.scala:26)
	at com.datastax.cdm.job.Migrate$.$anonfun$execute$2$adapted(Migrate.scala:25)
	at com.datastax.spark.connector.cql.CassandraConnector.$anonfun$withSessionDo$1(CassandraConnector.scala:104)
	at com.datastax.spark.connector.cql.CassandraConnector.closeResourceAfterUse(CassandraConnector.scala:121)
	at com.datastax.spark.connector.cql.CassandraConnector.withSessionDo(CassandraConnector.scala:103)
	at com.datastax.cdm.job.Migrate$.$anonfun$execute$1(Migrate.scala:25)
	at com.datastax.cdm.job.Migrate$.$anonfun$execute$1$adapted(Migrate.scala:24)
	at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
	at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2(RDD.scala:1031)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2$adapted(RDD.scala:1031)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/07/30 23:40:53 INFO TaskSetManager: Lost task 3.0 in stage 0.0 (TID 3) on ip-10-166-69-127.us-west-2.compute.internal, executor driver: java.lang.IllegalArgumentException (Table not found: cdm,keyspace2) [duplicate 2]
24/07/30 23:40:53 INFO TaskSchedulerImpl: Cancelling stage 0
24/07/30 23:40:53 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage cancelled: Job aborted due to stage failure: Task 1 in stage 0.0 failed 1 times, most recent failure: Lost task 1.0 in stage 0.0 (TID 1) (ip-10-166-69-127.us-west-2.compute.internal executor driver): java.lang.IllegalArgumentException: Table not found: cdm,keyspace2
	at com.datastax.cdm.schema.CqlTable.setCqlMetadata(CqlTable.java:385)
	at com.datastax.cdm.schema.CqlTable.<init>(CqlTable.java:99)
	at com.datastax.cdm.cql.EnhancedSession.<init>(EnhancedSession.java:51)
	at com.datastax.cdm.job.AbstractJobSession.<init>(AbstractJobSession.java:72)
	at com.datastax.cdm.job.AbstractJobSession.<init>(AbstractJobSession.java:44)
	at com.datastax.cdm.job.CopyJobSession.<init>(CopyJobSession.java:47)
	at com.datastax.cdm.job.CopyJobSessionFactory.getInstance(CopyJobSessionFactory.java:28)
	at com.datastax.cdm.job.Migrate$.$anonfun$execute$3(Migrate.scala:27)
	at com.datastax.cdm.job.Migrate$.$anonfun$execute$3$adapted(Migrate.scala:26)
	at com.datastax.spark.connector.cql.CassandraConnector.$anonfun$withSessionDo$1(CassandraConnector.scala:104)
	at com.datastax.spark.connector.cql.CassandraConnector.closeResourceAfterUse(CassandraConnector.scala:121)
	at com.datastax.spark.connector.cql.CassandraConnector.withSessionDo(CassandraConnector.scala:103)
	at com.datastax.cdm.job.Migrate$.$anonfun$execute$2(Migrate.scala:26)
	at com.datastax.cdm.job.Migrate$.$anonfun$execute$2$adapted(Migrate.scala:25)
	at com.datastax.spark.connector.cql.CassandraConnector.$anonfun$withSessionDo$1(CassandraConnector.scala:104)
	at com.datastax.spark.connector.cql.CassandraConnector.closeResourceAfterUse(CassandraConnector.scala:121)
	at com.datastax.spark.connector.cql.CassandraConnector.withSessionDo(CassandraConnector.scala:103)
	at com.datastax.cdm.job.Migrate$.$anonfun$execute$1(Migrate.scala:25)
	at com.datastax.cdm.job.Migrate$.$anonfun$execute$1$adapted(Migrate.scala:24)
	at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
	at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2(RDD.scala:1031)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2$adapted(RDD.scala:1031)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)

Driver stacktrace:
24/07/30 23:40:53 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
24/07/30 23:40:53 INFO TaskSchedulerImpl: Stage 0 was cancelled
24/07/30 23:40:53 INFO DAGScheduler: ResultStage 0 (foreach at Migrate.scala:24) failed in 0.746 s due to Job aborted due to stage failure: Task 1 in stage 0.0 failed 1 times, most recent failure: Lost task 1.0 in stage 0.0 (TID 1) (ip-10-166-69-127.us-west-2.compute.internal executor driver): java.lang.IllegalArgumentException: Table not found: cdm,keyspace2
	at com.datastax.cdm.schema.CqlTable.setCqlMetadata(CqlTable.java:385)
	at com.datastax.cdm.schema.CqlTable.<init>(CqlTable.java:99)
	at com.datastax.cdm.cql.EnhancedSession.<init>(EnhancedSession.java:51)
	at com.datastax.cdm.job.AbstractJobSession.<init>(AbstractJobSession.java:72)
	at com.datastax.cdm.job.AbstractJobSession.<init>(AbstractJobSession.java:44)
	at com.datastax.cdm.job.CopyJobSession.<init>(CopyJobSession.java:47)
	at com.datastax.cdm.job.CopyJobSessionFactory.getInstance(CopyJobSessionFactory.java:28)
	at com.datastax.cdm.job.Migrate$.$anonfun$execute$3(Migrate.scala:27)
	at com.datastax.cdm.job.Migrate$.$anonfun$execute$3$adapted(Migrate.scala:26)
	at com.datastax.spark.connector.cql.CassandraConnector.$anonfun$withSessionDo$1(CassandraConnector.scala:104)
	at com.datastax.spark.connector.cql.CassandraConnector.closeResourceAfterUse(CassandraConnector.scala:121)
	at com.datastax.spark.connector.cql.CassandraConnector.withSessionDo(CassandraConnector.scala:103)
	at com.datastax.cdm.job.Migrate$.$anonfun$execute$2(Migrate.scala:26)
	at com.datastax.cdm.job.Migrate$.$anonfun$execute$2$adapted(Migrate.scala:25)
	at com.datastax.spark.connector.cql.CassandraConnector.$anonfun$withSessionDo$1(CassandraConnector.scala:104)
	at com.datastax.spark.connector.cql.CassandraConnector.closeResourceAfterUse(CassandraConnector.scala:121)
	at com.datastax.spark.connector.cql.CassandraConnector.withSessionDo(CassandraConnector.scala:103)
	at com.datastax.cdm.job.Migrate$.$anonfun$execute$1(Migrate.scala:25)
	at com.datastax.cdm.job.Migrate$.$anonfun$execute$1$adapted(Migrate.scala:24)
	at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
	at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2(RDD.scala:1031)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2$adapted(RDD.scala:1031)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)

Driver stacktrace:
24/07/30 23:40:53 INFO TaskSetManager: Lost task 2.0 in stage 0.0 (TID 2) on ip-10-166-69-127.us-west-2.compute.internal, executor driver: java.lang.IllegalArgumentException (Table not found: cdm,keyspace2) [duplicate 3]
24/07/30 23:40:53 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
24/07/30 23:40:53 INFO DAGScheduler: Job 0 failed: foreach at Migrate.scala:24, took 0.838172 s
Exception in thread "main" org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 0.0 failed 1 times, most recent failure: Lost task 1.0 in stage 0.0 (TID 1) (ip-10-166-69-127.us-west-2.compute.internal executor driver): java.lang.IllegalArgumentException: Table not found: cdm,keyspace2
	at com.datastax.cdm.schema.CqlTable.setCqlMetadata(CqlTable.java:385)
	at com.datastax.cdm.schema.CqlTable.<init>(CqlTable.java:99)
	at com.datastax.cdm.cql.EnhancedSession.<init>(EnhancedSession.java:51)
	at com.datastax.cdm.job.AbstractJobSession.<init>(AbstractJobSession.java:72)
	at com.datastax.cdm.job.AbstractJobSession.<init>(AbstractJobSession.java:44)
	at com.datastax.cdm.job.CopyJobSession.<init>(CopyJobSession.java:47)
	at com.datastax.cdm.job.CopyJobSessionFactory.getInstance(CopyJobSessionFactory.java:28)
	at com.datastax.cdm.job.Migrate$.$anonfun$execute$3(Migrate.scala:27)
	at com.datastax.cdm.job.Migrate$.$anonfun$execute$3$adapted(Migrate.scala:26)
	at com.datastax.spark.connector.cql.CassandraConnector.$anonfun$withSessionDo$1(CassandraConnector.scala:104)
	at com.datastax.spark.connector.cql.CassandraConnector.closeResourceAfterUse(CassandraConnector.scala:121)
	at com.datastax.spark.connector.cql.CassandraConnector.withSessionDo(CassandraConnector.scala:103)
	at com.datastax.cdm.job.Migrate$.$anonfun$execute$2(Migrate.scala:26)
	at com.datastax.cdm.job.Migrate$.$anonfun$execute$2$adapted(Migrate.scala:25)
	at com.datastax.spark.connector.cql.CassandraConnector.$anonfun$withSessionDo$1(CassandraConnector.scala:104)
	at com.datastax.spark.connector.cql.CassandraConnector.closeResourceAfterUse(CassandraConnector.scala:121)
	at com.datastax.spark.connector.cql.CassandraConnector.withSessionDo(CassandraConnector.scala:103)
	at com.datastax.cdm.job.Migrate$.$anonfun$execute$1(Migrate.scala:25)
	at com.datastax.cdm.job.Migrate$.$anonfun$execute$1$adapted(Migrate.scala:24)
	at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
	at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2(RDD.scala:1031)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2$adapted(RDD.scala:1031)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)
	at scala.collection.immutable.List.foreach(List.scala:333)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)
	at scala.Option.foreach(Option.scala:437)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$1(RDD.scala:1031)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:410)
	at org.apache.spark.rdd.RDD.foreach(RDD.scala:1029)
	at com.datastax.cdm.job.Migrate$.execute(Migrate.scala:24)
	at com.datastax.cdm.job.Migrate$.delayedEndpoint$com$datastax$cdm$job$Migrate$1(Migrate.scala:20)
	at com.datastax.cdm.job.Migrate$delayedInit$body.apply(Migrate.scala:18)
	at scala.Function0.apply$mcV$sp(Function0.scala:39)
	at scala.Function0.apply$mcV$sp$(Function0.scala:39)
	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17)
	at scala.App.$anonfun$main$1(App.scala:76)
	at scala.App.$anonfun$main$1$adapted(App.scala:76)
	at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
	at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:926)
	at scala.App.main(App.scala:76)
	at scala.App.main$(App.scala:74)
	at com.datastax.cdm.job.BaseJob.main(BaseJob.scala:33)
	at com.datastax.cdm.job.Migrate.main(Migrate.scala)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
	at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:1029)
	at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:194)
	at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:217)
	at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)
	at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1120)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1129)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.lang.IllegalArgumentException: Table not found: cdm,keyspace2
	at com.datastax.cdm.schema.CqlTable.setCqlMetadata(CqlTable.java:385)
	at com.datastax.cdm.schema.CqlTable.<init>(CqlTable.java:99)
	at com.datastax.cdm.cql.EnhancedSession.<init>(EnhancedSession.java:51)
	at com.datastax.cdm.job.AbstractJobSession.<init>(AbstractJobSession.java:72)
	at com.datastax.cdm.job.AbstractJobSession.<init>(AbstractJobSession.java:44)
	at com.datastax.cdm.job.CopyJobSession.<init>(CopyJobSession.java:47)
	at com.datastax.cdm.job.CopyJobSessionFactory.getInstance(CopyJobSessionFactory.java:28)
	at com.datastax.cdm.job.Migrate$.$anonfun$execute$3(Migrate.scala:27)
	at com.datastax.cdm.job.Migrate$.$anonfun$execute$3$adapted(Migrate.scala:26)
	at com.datastax.spark.connector.cql.CassandraConnector.$anonfun$withSessionDo$1(CassandraConnector.scala:104)
	at com.datastax.spark.connector.cql.CassandraConnector.closeResourceAfterUse(CassandraConnector.scala:121)
	at com.datastax.spark.connector.cql.CassandraConnector.withSessionDo(CassandraConnector.scala:103)
	at com.datastax.cdm.job.Migrate$.$anonfun$execute$2(Migrate.scala:26)
	at com.datastax.cdm.job.Migrate$.$anonfun$execute$2$adapted(Migrate.scala:25)
	at com.datastax.spark.connector.cql.CassandraConnector.$anonfun$withSessionDo$1(CassandraConnector.scala:104)
	at com.datastax.spark.connector.cql.CassandraConnector.closeResourceAfterUse(CassandraConnector.scala:121)
	at com.datastax.spark.connector.cql.CassandraConnector.withSessionDo(CassandraConnector.scala:103)
	at com.datastax.cdm.job.Migrate$.$anonfun$execute$1(Migrate.scala:25)
	at com.datastax.cdm.job.Migrate$.$anonfun$execute$1$adapted(Migrate.scala:24)
	at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
	at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2(RDD.scala:1031)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2$adapted(RDD.scala:1031)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/07/30 23:40:53 INFO SparkContext: Invoking stop() from shutdown hook
24/07/30 23:40:53 INFO SparkContext: SparkContext is stopping with exitCode 0.
24/07/30 23:40:53 INFO SparkUI: Stopped Spark web UI at http://ip-10-166-69-127.us-west-2.compute.internal:4040
24/07/30 23:40:53 INFO CassandraConnector: Disconnected from Cassandra cluster.
24/07/30 23:40:53 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
24/07/30 23:40:53 INFO CassandraConnector: Disconnected from Cassandra cluster.
24/07/30 23:40:53 INFO SerialShutdownHooks: Successfully executed shutdown hook: Clearing session cache for C* connector
24/07/30 23:40:53 INFO MemoryStore: MemoryStore cleared
24/07/30 23:40:53 INFO BlockManager: BlockManager stopped
24/07/30 23:40:53 INFO BlockManagerMaster: BlockManagerMaster stopped
24/07/30 23:40:53 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
24/07/30 23:40:53 INFO SparkContext: Successfully stopped SparkContext
24/07/30 23:40:53 INFO ShutdownHookManager: Shutdown hook called
24/07/30 23:40:53 INFO ShutdownHookManager: Deleting directory /tmp/spark-33c3d76f-cbde-49a5-b43d-0dd705a6b484
24/07/30 23:40:53 INFO ShutdownHookManager: Deleting directory /tmp/spark-872ba871-57e9-4d76-bed2-7646573f311a
