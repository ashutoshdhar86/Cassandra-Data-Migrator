24/07/30 23:20:56 INFO Migrate$: ################################################################################################
24/07/30 23:20:56 INFO Migrate$: ###                                  Migrate Job - Starting                                  ###
24/07/30 23:20:56 INFO Migrate$: ################################################################################################
24/07/30 23:20:56 INFO SparkContext: Running Spark version 3.5.1
24/07/30 23:20:56 INFO SparkContext: OS info Linux, 4.18.0-553.6.1.el8.x86_64, amd64
24/07/30 23:20:56 INFO SparkContext: Java version 11.0.20.1
24/07/30 23:20:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
24/07/30 23:20:56 INFO ResourceUtils: ==============================================================
24/07/30 23:20:56 INFO ResourceUtils: No custom resources configured for spark.driver.
24/07/30 23:20:56 INFO ResourceUtils: ==============================================================
24/07/30 23:20:56 INFO SparkContext: Submitted application: Migrate Job
24/07/30 23:20:56 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
24/07/30 23:20:56 INFO ResourceProfile: Limiting resource is cpu
24/07/30 23:20:56 INFO ResourceProfileManager: Added ResourceProfile id: 0
24/07/30 23:20:56 INFO SecurityManager: Changing view acls to: automaton
24/07/30 23:20:56 INFO SecurityManager: Changing modify acls to: automaton
24/07/30 23:20:56 INFO SecurityManager: Changing view acls groups to: 
24/07/30 23:20:56 INFO SecurityManager: Changing modify acls groups to: 
24/07/30 23:20:56 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: automaton; groups with view permissions: EMPTY; users with modify permissions: automaton; groups with modify permissions: EMPTY
24/07/30 23:20:57 INFO Utils: Successfully started service 'sparkDriver' on port 34757.
24/07/30 23:20:57 INFO SparkEnv: Registering MapOutputTracker
24/07/30 23:20:57 INFO SparkEnv: Registering BlockManagerMaster
24/07/30 23:20:57 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
24/07/30 23:20:57 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
24/07/30 23:20:57 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
24/07/30 23:20:57 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-95356c86-634c-4d4b-b9b8-782570d701ef
24/07/30 23:20:57 INFO MemoryStore: MemoryStore started with capacity 1048.8 MiB
24/07/30 23:20:57 INFO SparkEnv: Registering OutputCommitCoordinator
24/07/30 23:20:57 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
24/07/30 23:20:57 INFO Utils: Successfully started service 'SparkUI' on port 4040.
24/07/30 23:20:57 INFO SparkContext: Added JAR file:/home/automaton/cassandra-data-migrator-4.2.0.jar at spark://ip-10-166-69-127.us-west-2.compute.internal:34757/jars/cassandra-data-migrator-4.2.0.jar with timestamp 1722381656181
24/07/30 23:20:57 INFO Executor: Starting executor ID driver on host ip-10-166-69-127.us-west-2.compute.internal
24/07/30 23:20:57 INFO Executor: OS info Linux, 4.18.0-553.6.1.el8.x86_64, amd64
24/07/30 23:20:57 INFO Executor: Java version 11.0.20.1
24/07/30 23:20:57 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
24/07/30 23:20:57 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@6e3ecf5c for default.
24/07/30 23:20:57 INFO Executor: Fetching spark://ip-10-166-69-127.us-west-2.compute.internal:34757/jars/cassandra-data-migrator-4.2.0.jar with timestamp 1722381656181
24/07/30 23:20:57 INFO TransportClientFactory: Successfully created connection to ip-10-166-69-127.us-west-2.compute.internal/10.166.69.127:34757 after 63 ms (0 ms spent in bootstraps)
24/07/30 23:20:58 INFO Utils: Fetching spark://ip-10-166-69-127.us-west-2.compute.internal:34757/jars/cassandra-data-migrator-4.2.0.jar to /tmp/spark-7e635238-2c0c-439a-bbc4-132961dc41cc/userFiles-8cf456b3-1694-4de8-b1d8-af88887cb2ca/fetchFileTemp5610682743211127803.tmp
24/07/30 23:20:58 INFO Executor: Adding file:/tmp/spark-7e635238-2c0c-439a-bbc4-132961dc41cc/userFiles-8cf456b3-1694-4de8-b1d8-af88887cb2ca/cassandra-data-migrator-4.2.0.jar to class loader default
24/07/30 23:20:58 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42927.
24/07/30 23:20:58 INFO NettyBlockTransferService: Server created on ip-10-166-69-127.us-west-2.compute.internal:42927
24/07/30 23:20:58 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
24/07/30 23:20:58 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, ip-10-166-69-127.us-west-2.compute.internal, 42927, None)
24/07/30 23:20:58 INFO BlockManagerMasterEndpoint: Registering block manager ip-10-166-69-127.us-west-2.compute.internal:42927 with 1048.8 MiB RAM, BlockManagerId(driver, ip-10-166-69-127.us-west-2.compute.internal, 42927, None)
24/07/30 23:20:58 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, ip-10-166-69-127.us-west-2.compute.internal, 42927, None)
24/07/30 23:20:58 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, ip-10-166-69-127.us-west-2.compute.internal, 42927, None)
24/07/30 23:20:58 INFO PropertyHelper: Processing explicitly set and known sparkConf properties
24/07/30 23:20:58 INFO PropertyHelper: Known property [spark.cdm.autocorrect.missing] is configured with value [false] and is type [BOOLEAN]
24/07/30 23:20:58 INFO PropertyHelper: Known property [spark.cdm.connect.origin.host] is configured with value [10.166.65.33] and is type [STRING]
24/07/30 23:20:58 INFO PropertyHelper: Known property [spark.cdm.perfops.numParts] is configured with value [10] and is type [NUMBER]
24/07/30 23:20:58 INFO PropertyHelper: Known property [spark.cdm.connect.origin.password] is configured with value [********] and is type [STRING]
24/07/30 23:20:58 INFO PropertyHelper: Known property [spark.cdm.connect.target.password] is configured with value [********] and is type [STRING]
24/07/30 23:20:58 INFO PropertyHelper: Known property [spark.cdm.connect.target.username] is configured with value [cassandra] and is type [STRING]
24/07/30 23:20:58 INFO PropertyHelper: Known property [spark.cdm.autocorrect.mismatch] is configured with value [false] and is type [BOOLEAN]
24/07/30 23:20:58 INFO PropertyHelper: Known property [spark.cdm.connect.target.host] is configured with value [10.166.65.248] and is type [STRING]
24/07/30 23:20:58 INFO PropertyHelper: Known property [spark.cdm.connect.target.port] is configured with value [9042] and is type [NUMBER]
24/07/30 23:20:58 INFO PropertyHelper: Known property [spark.cdm.connect.origin.port] is configured with value [9042] and is type [NUMBER]
24/07/30 23:20:58 INFO PropertyHelper: Known property [spark.cdm.connect.origin.username] is configured with value [cassandra] and is type [STRING]
24/07/30 23:20:58 INFO PropertyHelper: Known property [spark.cdm.schema.origin.keyspaceTable] is configured with value [ks1.cdm_type] and is type [STRING]
24/07/30 23:20:58 INFO PropertyHelper: Adding any missing known properties that have default values
24/07/30 23:20:58 INFO ConnectionFetcher: PARAM --  SSL Enabled: false
24/07/30 23:20:58 INFO ConnectionFetcher: Connecting to ORIGIN at 10.166.65.33:9042
24/07/30 23:21:00 INFO ConnectionFetcher: PARAM --  SSL Enabled: false
24/07/30 23:21:00 INFO ConnectionFetcher: Connecting to TARGET at 10.166.65.248:9042
24/07/30 23:21:00 INFO DefaultMavenCoordinates: Apache Cassandra Java Driver (org.apache.cassandra:java-driver-core-shaded) version 4.18.1
24/07/30 23:21:00 INFO CqlPrepareAsyncProcessor: Adding handler to invalidate cached prepared statements on type changes
24/07/30 23:21:01 INFO Clock: Using native clock for microsecond precision
24/07/30 23:21:01 WARN PlainTextAuthProviderBase: [] /10.166.65.33:9042 did not send an authentication challenge; This is suspicious because the driver expects authentication
24/07/30 23:21:01 WARN DefaultTopologyMonitor: [s0] Unable to determine broadcast RPC port.  Trying to fall back to port used by the control connection.
24/07/30 23:21:02 WARN PlainTextAuthProviderBase: [] /10.166.65.33:9042 did not send an authentication challenge; This is suspicious because the driver expects authentication
24/07/30 23:21:02 INFO CassandraConnector: Connected to Cassandra cluster.
24/07/30 23:21:02 INFO Migrate$: PARAM -- Min Partition: -9223372036854775808
24/07/30 23:21:02 INFO Migrate$: PARAM -- Max Partition: 9223372036854775807
24/07/30 23:21:02 INFO Migrate$: PARAM -- Number of Splits : 10
24/07/30 23:21:02 INFO Migrate$: PARAM -- Coverage Percent: 100
24/07/30 23:21:02 INFO SplitPartitions: ThreadID: 1 Splitting min: -9223372036854775808 max: 9223372036854775807
24/07/30 23:21:02 INFO Migrate$: PARAM Calculated -- Total Partitions: 10
24/07/30 23:21:02 INFO Migrate$: Spark parallelize created : 10 slices!
24/07/30 23:21:03 INFO SparkContext: Starting job: foreach at Migrate.scala:24
24/07/30 23:21:03 INFO DAGScheduler: Got job 0 (foreach at Migrate.scala:24) with 10 output partitions
24/07/30 23:21:03 INFO DAGScheduler: Final stage: ResultStage 0 (foreach at Migrate.scala:24)
24/07/30 23:21:03 INFO DAGScheduler: Parents of final stage: List()
24/07/30 23:21:03 INFO DAGScheduler: Missing parents: List()
24/07/30 23:21:03 INFO DAGScheduler: Submitting ResultStage 0 (ParallelCollectionRDD[0] at parallelize at BaseJob.scala:90), which has no missing parents
24/07/30 23:21:03 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 3.1 KiB, free 1048.8 MiB)
24/07/30 23:21:03 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1854.0 B, free 1048.8 MiB)
24/07/30 23:21:03 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on ip-10-166-69-127.us-west-2.compute.internal:42927 (size: 1854.0 B, free: 1048.8 MiB)
24/07/30 23:21:03 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1585
24/07/30 23:21:03 INFO DAGScheduler: Submitting 10 missing tasks from ResultStage 0 (ParallelCollectionRDD[0] at parallelize at BaseJob.scala:90) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9))
24/07/30 23:21:03 INFO TaskSchedulerImpl: Adding task set 0.0 with 10 tasks resource profile 0
24/07/30 23:21:03 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (ip-10-166-69-127.us-west-2.compute.internal, executor driver, partition 0, PROCESS_LOCAL, 8324 bytes) 
24/07/30 23:21:03 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (ip-10-166-69-127.us-west-2.compute.internal, executor driver, partition 1, PROCESS_LOCAL, 8317 bytes) 
24/07/30 23:21:03 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
24/07/30 23:21:03 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
24/07/30 23:21:03 INFO CqlPrepareAsyncProcessor: Adding handler to invalidate cached prepared statements on type changes
24/07/30 23:21:03 INFO Clock: Using native clock for microsecond precision
24/07/30 23:21:03 WARN PlainTextAuthProviderBase: [] /10.166.65.248:9042 did not send an authentication challenge; This is suspicious because the driver expects authentication
24/07/30 23:21:03 WARN PlainTextAuthProviderBase: [] /10.166.65.248:9042 did not send an authentication challenge; This is suspicious because the driver expects authentication
24/07/30 23:21:03 INFO CassandraConnector: Connected to Cassandra cluster.
24/07/30 23:21:03 INFO CopyJobSession: PARAM -- Max Retries: 0
24/07/30 23:21:03 INFO CopyJobSession: PARAM -- Partition file input: ./ks1.cdm_type_partitions.csv
24/07/30 23:21:03 INFO CopyJobSession: PARAM -- Partition file output: ./ks1.cdm_type_partitions.csv
24/07/30 23:21:03 INFO CopyJobSession: PARAM -- Origin Rate Limit: 20000.0
24/07/30 23:21:03 INFO CopyJobSession: PARAM -- Target Rate Limit: 40000.0
24/07/30 23:21:03 INFO WritetimeTTL: PARAM -- Automatic TTLCols: [c3, c4]
24/07/30 23:21:03 INFO WritetimeTTL: PARAM -- Automatic WriteTimestampCols: [c3, c4]
24/07/30 23:21:03 INFO WritetimeTTL: Feature WritetimeTTL is enabled
24/07/30 23:21:03 INFO CopyJobSession: CQL -- origin select: SELECT pk1,ck1,c3,c4,TTL(c3),TTL(c4),WRITETIME(c3),WRITETIME(c4) FROM ks1.cdm_type WHERE TOKEN(pk1) >= ? AND TOKEN(pk1) <= ? ALLOW FILTERING
24/07/30 23:21:03 INFO CopyJobSession: CQL -- target select: SELECT pk1,ck1,c3,c4 FROM ks1.cdm_type WHERE pk1=? AND ck1=?
24/07/30 23:21:03 INFO CopyJobSession: CQL -- target upsert: INSERT INTO ks1.cdm_type (pk1,ck1,c3,c4) VALUES (?,?,?,?) USING TTL ? AND TIMESTAMP ?
24/07/30 23:21:03 INFO CopyJobSession: ThreadID: 63 Processing min: -3689348814741910322 max: -1844674407370955161
24/07/30 23:21:03 INFO CopyJobSession: ThreadID: 64 Processing min: -1844674407370955160 max: 1
24/07/30 23:21:04 ERROR CopyJobSession: Error occurred during Attempt#: 1
com.datastax.oss.driver.api.core.type.codec.CodecNotFoundException: Codec not found for requested operation: [INT <-> java.lang.String]
	at com.datastax.oss.driver.internal.core.type.codec.registry.CachingCodecRegistry.createCodec(CachingCodecRegistry.java:667)
	at com.datastax.oss.driver.internal.core.type.codec.registry.DefaultCodecRegistry$1.load(DefaultCodecRegistry.java:97)
	at com.datastax.oss.driver.internal.core.type.codec.registry.DefaultCodecRegistry$1.load(DefaultCodecRegistry.java:94)
	at com.datastax.oss.driver.shaded.guava.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3527)
	at com.datastax.oss.driver.shaded.guava.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2276)
	at com.datastax.oss.driver.shaded.guava.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2154)
	at com.datastax.oss.driver.shaded.guava.common.cache.LocalCache$Segment.get(LocalCache.java:2044)
	at com.datastax.oss.driver.shaded.guava.common.cache.LocalCache.get(LocalCache.java:3951)
	at com.datastax.oss.driver.shaded.guava.common.cache.LocalCache.getOrLoad(LocalCache.java:3973)
	at com.datastax.oss.driver.shaded.guava.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4957)
	at com.datastax.oss.driver.shaded.guava.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4963)
	at com.datastax.oss.driver.internal.core.type.codec.registry.DefaultCodecRegistry.getCachedCodec(DefaultCodecRegistry.java:119)
	at com.datastax.oss.driver.internal.core.type.codec.registry.CachingCodecRegistry.codecFor(CachingCodecRegistry.java:220)
	at com.datastax.cdm.data.CqlConversion.convert_CODEC(CqlConversion.java:246)
	at com.datastax.cdm.data.CqlConversion.convert_ONE(CqlConversion.java:227)
	at com.datastax.cdm.data.CqlConversion.convert(CqlConversion.java:99)
	at com.datastax.cdm.schema.CqlTable.getAndConvertData(CqlTable.java:305)
	at com.datastax.cdm.data.PKFactory.getTargetPKValuesFromOriginColumnLookupMethod(PKFactory.java:241)
	at com.datastax.cdm.data.PKFactory.getTargetPK(PKFactory.java:98)
	at com.datastax.cdm.job.CopyJobSession.getDataAndInsert(CopyJobSession.java:86)
	at com.datastax.cdm.job.CopyJobSession.processSlice(CopyJobSession.java:62)
	at com.datastax.cdm.job.CopyJobSession.processSlice(CopyJobSession.java:36)
	at com.datastax.cdm.job.Migrate$.$anonfun$execute$3(Migrate.scala:28)
	at com.datastax.cdm.job.Migrate$.$anonfun$execute$3$adapted(Migrate.scala:26)
	at com.datastax.spark.connector.cql.CassandraConnector.$anonfun$withSessionDo$1(CassandraConnector.scala:104)
	at com.datastax.spark.connector.cql.CassandraConnector.closeResourceAfterUse(CassandraConnector.scala:121)
	at com.datastax.spark.connector.cql.CassandraConnector.withSessionDo(CassandraConnector.scala:103)
	at com.datastax.cdm.job.Migrate$.$anonfun$execute$2(Migrate.scala:26)
	at com.datastax.cdm.job.Migrate$.$anonfun$execute$2$adapted(Migrate.scala:25)
	at com.datastax.spark.connector.cql.CassandraConnector.$anonfun$withSessionDo$1(CassandraConnector.scala:104)
	at com.datastax.spark.connector.cql.CassandraConnector.closeResourceAfterUse(CassandraConnector.scala:121)
	at com.datastax.spark.connector.cql.CassandraConnector.withSessionDo(CassandraConnector.scala:103)
	at com.datastax.cdm.job.Migrate$.$anonfun$execute$1(Migrate.scala:25)
	at com.datastax.cdm.job.Migrate$.$anonfun$execute$1$adapted(Migrate.scala:24)
	at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
	at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2(RDD.scala:1031)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2$adapted(RDD.scala:1031)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/07/30 23:21:04 ERROR CopyJobSession: Error with PartitionRange -- ThreadID: 64 Processing min: -1844674407370955160 max: 1 -- Attempt# 1
24/07/30 23:21:04 ERROR CopyJobSession: Error stats ERROR=1, SKIPPED=0, WRITE=0, READ=1, UNFLUSHED=0
24/07/30 23:21:04 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 981 bytes result sent to driver
24/07/30 23:21:04 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 981 bytes result sent to driver
24/07/30 23:21:04 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2) (ip-10-166-69-127.us-west-2.compute.internal, executor driver, partition 2, PROCESS_LOCAL, 8324 bytes) 
24/07/30 23:21:04 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
24/07/30 23:21:04 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3) (ip-10-166-69-127.us-west-2.compute.internal, executor driver, partition 3, PROCESS_LOCAL, 8324 bytes) 
24/07/30 23:21:04 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
24/07/30 23:21:04 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 607 ms on ip-10-166-69-127.us-west-2.compute.internal (executor driver) (1/10)
24/07/30 23:21:04 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 566 ms on ip-10-166-69-127.us-west-2.compute.internal (executor driver) (2/10)
24/07/30 23:21:04 INFO CopyJobSession: ThreadID: 63 Processing min: 5534023222112865488 max: 7378697629483820649
24/07/30 23:21:04 INFO CopyJobSession: ThreadID: 64 Processing min: 3689348814741910326 max: 5534023222112865487
24/07/30 23:21:04 ERROR CopyJobSession: Error occurred during Attempt#: 1
com.datastax.oss.driver.api.core.type.codec.CodecNotFoundException: Codec not found for requested operation: [INT <-> java.lang.String]
	at com.datastax.oss.driver.internal.core.type.codec.registry.CachingCodecRegistry.createCodec(CachingCodecRegistry.java:667)
	at com.datastax.oss.driver.internal.core.type.codec.registry.DefaultCodecRegistry$1.load(DefaultCodecRegistry.java:97)
	at com.datastax.oss.driver.internal.core.type.codec.registry.DefaultCodecRegistry$1.load(DefaultCodecRegistry.java:94)
	at com.datastax.oss.driver.shaded.guava.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3527)
	at com.datastax.oss.driver.shaded.guava.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2276)
	at com.datastax.oss.driver.shaded.guava.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2154)
	at com.datastax.oss.driver.shaded.guava.common.cache.LocalCache$Segment.get(LocalCache.java:2044)
	at com.datastax.oss.driver.shaded.guava.common.cache.LocalCache.get(LocalCache.java:3951)
	at com.datastax.oss.driver.shaded.guava.common.cache.LocalCache.getOrLoad(LocalCache.java:3973)
	at com.datastax.oss.driver.shaded.guava.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4957)
	at com.datastax.oss.driver.shaded.guava.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4963)
	at com.datastax.oss.driver.internal.core.type.codec.registry.DefaultCodecRegistry.getCachedCodec(DefaultCodecRegistry.java:119)
	at com.datastax.oss.driver.internal.core.type.codec.registry.CachingCodecRegistry.codecFor(CachingCodecRegistry.java:220)
	at com.datastax.cdm.data.CqlConversion.convert_CODEC(CqlConversion.java:246)
	at com.datastax.cdm.data.CqlConversion.convert_ONE(CqlConversion.java:227)
	at com.datastax.cdm.data.CqlConversion.convert(CqlConversion.java:99)
	at com.datastax.cdm.schema.CqlTable.getAndConvertData(CqlTable.java:305)
	at com.datastax.cdm.data.PKFactory.getTargetPKValuesFromOriginColumnLookupMethod(PKFactory.java:241)
	at com.datastax.cdm.data.PKFactory.getTargetPK(PKFactory.java:98)
	at com.datastax.cdm.job.CopyJobSession.getDataAndInsert(CopyJobSession.java:86)
	at com.datastax.cdm.job.CopyJobSession.processSlice(CopyJobSession.java:62)
	at com.datastax.cdm.job.CopyJobSession.processSlice(CopyJobSession.java:36)
	at com.datastax.cdm.job.Migrate$.$anonfun$execute$3(Migrate.scala:28)
	at com.datastax.cdm.job.Migrate$.$anonfun$execute$3$adapted(Migrate.scala:26)
	at com.datastax.spark.connector.cql.CassandraConnector.$anonfun$withSessionDo$1(CassandraConnector.scala:104)
	at com.datastax.spark.connector.cql.CassandraConnector.closeResourceAfterUse(CassandraConnector.scala:121)
	at com.datastax.spark.connector.cql.CassandraConnector.withSessionDo(CassandraConnector.scala:103)
	at com.datastax.cdm.job.Migrate$.$anonfun$execute$2(Migrate.scala:26)
	at com.datastax.cdm.job.Migrate$.$anonfun$execute$2$adapted(Migrate.scala:25)
	at com.datastax.spark.connector.cql.CassandraConnector.$anonfun$withSessionDo$1(CassandraConnector.scala:104)
	at com.datastax.spark.connector.cql.CassandraConnector.closeResourceAfterUse(CassandraConnector.scala:121)
	at com.datastax.spark.connector.cql.CassandraConnector.withSessionDo(CassandraConnector.scala:103)
	at com.datastax.cdm.job.Migrate$.$anonfun$execute$1(Migrate.scala:25)
	at com.datastax.cdm.job.Migrate$.$anonfun$execute$1$adapted(Migrate.scala:24)
	at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
	at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2(RDD.scala:1031)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2$adapted(RDD.scala:1031)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/07/30 23:21:04 ERROR CopyJobSession: Error with PartitionRange -- ThreadID: 64 Processing min: 3689348814741910326 max: 5534023222112865487 -- Attempt# 1
24/07/30 23:21:04 ERROR CopyJobSession: Error stats ERROR=1, SKIPPED=0, WRITE=0, READ=1, UNFLUSHED=0
24/07/30 23:21:04 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 938 bytes result sent to driver
24/07/30 23:21:04 INFO TaskSetManager: Starting task 4.0 in stage 0.0 (TID 4) (ip-10-166-69-127.us-west-2.compute.internal, executor driver, partition 4, PROCESS_LOCAL, 8324 bytes) 
24/07/30 23:21:04 INFO Executor: Running task 4.0 in stage 0.0 (TID 4)
24/07/30 23:21:04 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 68 ms on ip-10-166-69-127.us-west-2.compute.internal (executor driver) (3/10)
24/07/30 23:21:04 INFO CopyJobSession: ThreadID: 63 Processing min: 1844674407370955164 max: 3689348814741910325
24/07/30 23:21:04 INFO Executor: Finished task 3.0 in stage 0.0 (TID 3). 938 bytes result sent to driver
24/07/30 23:21:04 INFO TaskSetManager: Starting task 5.0 in stage 0.0 (TID 5) (ip-10-166-69-127.us-west-2.compute.internal, executor driver, partition 5, PROCESS_LOCAL, 8324 bytes) 
24/07/30 23:21:04 INFO Executor: Running task 5.0 in stage 0.0 (TID 5)
24/07/30 23:21:04 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 82 ms on ip-10-166-69-127.us-west-2.compute.internal (executor driver) (4/10)
24/07/30 23:21:04 INFO Executor: Finished task 4.0 in stage 0.0 (TID 4). 981 bytes result sent to driver
24/07/30 23:21:04 INFO CopyJobSession: ThreadID: 64 Processing min: -7378697629483820646 max: -5534023222112865485
24/07/30 23:21:04 INFO TaskSetManager: Starting task 6.0 in stage 0.0 (TID 6) (ip-10-166-69-127.us-west-2.compute.internal, executor driver, partition 6, PROCESS_LOCAL, 8324 bytes) 
24/07/30 23:21:04 INFO TaskSetManager: Finished task 4.0 in stage 0.0 (TID 4) in 40 ms on ip-10-166-69-127.us-west-2.compute.internal (executor driver) (5/10)
24/07/30 23:21:04 INFO Executor: Running task 6.0 in stage 0.0 (TID 6)
24/07/30 23:21:04 INFO CopyJobSession: ThreadID: 63 Processing min: -5534023222112865484 max: -3689348814741910323
24/07/30 23:21:04 INFO Executor: Finished task 5.0 in stage 0.0 (TID 5). 938 bytes result sent to driver
24/07/30 23:21:04 INFO TaskSetManager: Starting task 7.0 in stage 0.0 (TID 7) (ip-10-166-69-127.us-west-2.compute.internal, executor driver, partition 7, PROCESS_LOCAL, 8324 bytes) 
24/07/30 23:21:04 INFO TaskSetManager: Finished task 5.0 in stage 0.0 (TID 5) in 34 ms on ip-10-166-69-127.us-west-2.compute.internal (executor driver) (6/10)
24/07/30 23:21:04 INFO Executor: Running task 7.0 in stage 0.0 (TID 7)
24/07/30 23:21:04 INFO Executor: Finished task 6.0 in stage 0.0 (TID 6). 938 bytes result sent to driver
24/07/30 23:21:04 INFO TaskSetManager: Starting task 8.0 in stage 0.0 (TID 8) (ip-10-166-69-127.us-west-2.compute.internal, executor driver, partition 8, PROCESS_LOCAL, 8317 bytes) 
24/07/30 23:21:04 INFO Executor: Running task 8.0 in stage 0.0 (TID 8)
24/07/30 23:21:04 INFO TaskSetManager: Finished task 6.0 in stage 0.0 (TID 6) in 32 ms on ip-10-166-69-127.us-west-2.compute.internal (executor driver) (7/10)
24/07/30 23:21:04 INFO CopyJobSession: ThreadID: 64 Processing min: -9223372036854775808 max: -7378697629483820647
24/07/30 23:21:04 INFO CopyJobSession: ThreadID: 63 Processing min: 2 max: 1844674407370955163
24/07/30 23:21:04 INFO Executor: Finished task 7.0 in stage 0.0 (TID 7). 981 bytes result sent to driver
24/07/30 23:21:04 INFO TaskSetManager: Starting task 9.0 in stage 0.0 (TID 9) (ip-10-166-69-127.us-west-2.compute.internal, executor driver, partition 9, PROCESS_LOCAL, 8324 bytes) 
24/07/30 23:21:04 INFO TaskSetManager: Finished task 7.0 in stage 0.0 (TID 7) in 43 ms on ip-10-166-69-127.us-west-2.compute.internal (executor driver) (8/10)
24/07/30 23:21:04 INFO Executor: Finished task 8.0 in stage 0.0 (TID 8). 938 bytes result sent to driver
24/07/30 23:21:04 INFO Executor: Running task 9.0 in stage 0.0 (TID 9)
24/07/30 23:21:04 INFO TaskSetManager: Finished task 8.0 in stage 0.0 (TID 8) in 36 ms on ip-10-166-69-127.us-west-2.compute.internal (executor driver) (9/10)
24/07/30 23:21:04 INFO CopyJobSession: ThreadID: 64 Processing min: 7378697629483820650 max: 9223372036854775807
24/07/30 23:21:04 ERROR CopyJobSession: Error occurred during Attempt#: 1
com.datastax.oss.driver.api.core.type.codec.CodecNotFoundException: Codec not found for requested operation: [INT <-> java.lang.String]
	at com.datastax.oss.driver.internal.core.type.codec.registry.CachingCodecRegistry.createCodec(CachingCodecRegistry.java:667)
	at com.datastax.oss.driver.internal.core.type.codec.registry.DefaultCodecRegistry$1.load(DefaultCodecRegistry.java:97)
	at com.datastax.oss.driver.internal.core.type.codec.registry.DefaultCodecRegistry$1.load(DefaultCodecRegistry.java:94)
	at com.datastax.oss.driver.shaded.guava.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3527)
	at com.datastax.oss.driver.shaded.guava.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2276)
	at com.datastax.oss.driver.shaded.guava.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2154)
	at com.datastax.oss.driver.shaded.guava.common.cache.LocalCache$Segment.get(LocalCache.java:2044)
	at com.datastax.oss.driver.shaded.guava.common.cache.LocalCache.get(LocalCache.java:3951)
	at com.datastax.oss.driver.shaded.guava.common.cache.LocalCache.getOrLoad(LocalCache.java:3973)
	at com.datastax.oss.driver.shaded.guava.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4957)
	at com.datastax.oss.driver.shaded.guava.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4963)
	at com.datastax.oss.driver.internal.core.type.codec.registry.DefaultCodecRegistry.getCachedCodec(DefaultCodecRegistry.java:119)
	at com.datastax.oss.driver.internal.core.type.codec.registry.CachingCodecRegistry.codecFor(CachingCodecRegistry.java:220)
	at com.datastax.cdm.data.CqlConversion.convert_CODEC(CqlConversion.java:246)
	at com.datastax.cdm.data.CqlConversion.convert_ONE(CqlConversion.java:227)
	at com.datastax.cdm.data.CqlConversion.convert(CqlConversion.java:99)
	at com.datastax.cdm.schema.CqlTable.getAndConvertData(CqlTable.java:305)
	at com.datastax.cdm.data.PKFactory.getTargetPKValuesFromOriginColumnLookupMethod(PKFactory.java:241)
	at com.datastax.cdm.data.PKFactory.getTargetPK(PKFactory.java:98)
	at com.datastax.cdm.job.CopyJobSession.getDataAndInsert(CopyJobSession.java:86)
	at com.datastax.cdm.job.CopyJobSession.processSlice(CopyJobSession.java:62)
	at com.datastax.cdm.job.CopyJobSession.processSlice(CopyJobSession.java:36)
	at com.datastax.cdm.job.Migrate$.$anonfun$execute$3(Migrate.scala:28)
	at com.datastax.cdm.job.Migrate$.$anonfun$execute$3$adapted(Migrate.scala:26)
	at com.datastax.spark.connector.cql.CassandraConnector.$anonfun$withSessionDo$1(CassandraConnector.scala:104)
	at com.datastax.spark.connector.cql.CassandraConnector.closeResourceAfterUse(CassandraConnector.scala:121)
	at com.datastax.spark.connector.cql.CassandraConnector.withSessionDo(CassandraConnector.scala:103)
	at com.datastax.cdm.job.Migrate$.$anonfun$execute$2(Migrate.scala:26)
	at com.datastax.cdm.job.Migrate$.$anonfun$execute$2$adapted(Migrate.scala:25)
	at com.datastax.spark.connector.cql.CassandraConnector.$anonfun$withSessionDo$1(CassandraConnector.scala:104)
	at com.datastax.spark.connector.cql.CassandraConnector.closeResourceAfterUse(CassandraConnector.scala:121)
	at com.datastax.spark.connector.cql.CassandraConnector.withSessionDo(CassandraConnector.scala:103)
	at com.datastax.cdm.job.Migrate$.$anonfun$execute$1(Migrate.scala:25)
	at com.datastax.cdm.job.Migrate$.$anonfun$execute$1$adapted(Migrate.scala:24)
	at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
	at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2(RDD.scala:1031)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2$adapted(RDD.scala:1031)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/07/30 23:21:04 ERROR CopyJobSession: Error with PartitionRange -- ThreadID: 64 Processing min: 7378697629483820650 max: 9223372036854775807 -- Attempt# 1
24/07/30 23:21:04 ERROR CopyJobSession: Error stats ERROR=1, SKIPPED=0, WRITE=0, READ=1, UNFLUSHED=0
24/07/30 23:21:04 INFO Executor: Finished task 9.0 in stage 0.0 (TID 9). 938 bytes result sent to driver
24/07/30 23:21:04 INFO TaskSetManager: Finished task 9.0 in stage 0.0 (TID 9) in 31 ms on ip-10-166-69-127.us-west-2.compute.internal (executor driver) (10/10)
24/07/30 23:21:04 INFO DAGScheduler: ResultStage 0 (foreach at Migrate.scala:24) finished in 1.059 s
24/07/30 23:21:04 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
24/07/30 23:21:04 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
24/07/30 23:21:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
24/07/30 23:21:04 INFO DAGScheduler: Job 0 finished: foreach at Migrate.scala:24, took 1.156314 s
24/07/30 23:21:04 INFO JobCounter: ################################################################################################
24/07/30 23:21:04 INFO JobCounter: Final Read Record Count: 3
24/07/30 23:21:04 INFO JobCounter: Final Skipped Record Count: 0
24/07/30 23:21:04 INFO JobCounter: Final Write Record Count: 0
24/07/30 23:21:04 INFO JobCounter: Final Error Record Count: 3
24/07/30 23:21:04 INFO JobCounter: ################################################################################################
24/07/30 23:21:04 INFO SparkContext: SparkContext is stopping with exitCode 0.
24/07/30 23:21:04 INFO SparkUI: Stopped Spark web UI at http://ip-10-166-69-127.us-west-2.compute.internal:4040
24/07/30 23:21:04 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
24/07/30 23:21:04 INFO MemoryStore: MemoryStore cleared
24/07/30 23:21:04 INFO BlockManager: BlockManager stopped
24/07/30 23:21:04 INFO BlockManagerMaster: BlockManagerMaster stopped
24/07/30 23:21:04 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
24/07/30 23:21:04 INFO SparkContext: Successfully stopped SparkContext
24/07/30 23:21:04 INFO Migrate$: ################################################################################################
24/07/30 23:21:04 INFO Migrate$: ###                                  Migrate Job - Stopped                                   ###
24/07/30 23:21:04 INFO Migrate$: ################################################################################################
24/07/30 23:21:04 INFO ShutdownHookManager: Shutdown hook called
24/07/30 23:21:04 INFO ShutdownHookManager: Deleting directory /tmp/spark-caff78be-0d09-4e3f-865f-d14ef6d9a5d9
24/07/30 23:21:04 INFO ShutdownHookManager: Deleting directory /tmp/spark-7e635238-2c0c-439a-bbc4-132961dc41cc
24/07/30 23:21:04 INFO CassandraConnector: Disconnected from Cassandra cluster.
24/07/30 23:21:04 INFO CassandraConnector: Disconnected from Cassandra cluster.
24/07/30 23:21:04 INFO SerialShutdownHooks: Successfully executed shutdown hook: Clearing session cache for C* connector
